{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNet GYAFC.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUe23ikuwowY"
      },
      "source": [
        "!pip install transformers sentencepiece\n",
        "VERSION = \"1.8.1\"\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozVh90wYwKnU"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    DistilBertTokenizerFast,\n",
        "    XLNetTokenizer,\n",
        "    XLNetForSequenceClassification,\n",
        "    DistilBertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1haTb38wwPP"
      },
      "source": [
        "def data_read(data_path):\n",
        "    data = []\n",
        "    for file_name in glob(data_path):\n",
        "        with open(file_name) as f:\n",
        "            tmp_data = f.read()\n",
        "            data.extend(tmp_data.split('\\n'))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeyaEGI8wx78"
      },
      "source": [
        "path_formal = 'GYAFC_Corpus/*/{}/formal*'\n",
        "path_inform = 'GYAFC_Corpus/*/{}/informal*'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyE_fbANxDS-"
      },
      "source": [
        "file_path = 'GYAFC_Corpus.7z'\n",
        "if not os.path.isfile(file_path):\n",
        "    !wget -O GYAFC_Corpus.7z \"https://docs.google.com/uc?export=download&id=18KvT3MHnKtlHcFyna0044CxNbdgOLJXU\"\n",
        "    !7z x GYAFC_Corpus.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YE3k8XkxQ9W"
      },
      "source": [
        "data_train_form = data_read(path_formal.format('train'))\n",
        "data_train_inform = data_read(path_inform.format('train'))\n",
        "\n",
        "data_valid_form = data_read(path_formal.format('test'))\n",
        "data_valid_inform = data_read(path_inform.format('test'))\n",
        "\n",
        "data_test_form = data_read(path_formal.format('tune'))\n",
        "data_test_inform = data_read(path_inform.format('tune'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryZxvA2dxbpN"
      },
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sByl8VZZxlwN"
      },
      "source": [
        "def prep_dataset(formal, informal):\n",
        "    tuples = []\n",
        "    data = []\n",
        "    labels = []\n",
        "    formal = list(set(formal))\n",
        "    for sentence in formal:\n",
        "        data.append(sentence)\n",
        "        labels.append(0)\n",
        "    informal = list(set(informal))\n",
        "    for sentence in informal:\n",
        "        data.append(sentence)\n",
        "        labels.append(1)\n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKCl13RlxqWF"
      },
      "source": [
        "train_texts, train_labels = prep_dataset(data_train_form, data_train_inform)\n",
        "val_texts, val_labels = prep_dataset(data_valid_form, data_valid_inform)\n",
        "test_texts, test_labels = prep_dataset(data_test_form, data_test_inform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcjv71XAxeyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e69f71-ed82-4e8c-cd3b-4bd09264103c"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=24)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=24)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=24)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n",
            "  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8bwILekO-1K"
      },
      "source": [
        "import torch_xla.distributed.xla_multiprocessing as xmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEbG4_XO-Klj"
      },
      "source": [
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 2)\n",
        "# model.eval()\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjYaSfZH-1ja"
      },
      "source": [
        "class Formal_informal(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lakfs50a-X7i"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Compute metrics for Trainer\n",
        "    \"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    #_, _, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        #'macro f1': macro_f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "def train_nli(model, epochs=10, warmup_steps=200):\n",
        "    \"\"\"\n",
        "    This contains everything that must be done to train our models\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Loading datasets... \", end=\"\")\n",
        "    train_dataset = Formal_informal(train_encodings, train_labels)\n",
        "    val_dataset = Formal_informal(val_encodings, val_labels)\n",
        "    test_dataset = Formal_informal(test_encodings, test_labels)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',          # output directory\n",
        "        num_train_epochs=3,              # total number of training epochs\n",
        "        per_device_train_batch_size=16,  # batch size per device during training\n",
        "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "        weight_decay=0.01,               # strength of weight decay\n",
        "        logging_dir='./logs',            # directory for storing logs\n",
        "        logging_steps=500,\n",
        "        evaluation_strategy = 'steps',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "        args=training_args,                  # training arguments, defined above\n",
        "        train_dataset=train_dataset,         # training dataset\n",
        "        eval_dataset=val_dataset,             # evaluation dataset\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    trainer.place_model_on_device = False\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(\"nli_model/\")\n",
        "    tokenizer.save_pretrained(\"nli_model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34uTkFFSDcX9"
      },
      "source": [
        "!rm -R logs\n",
        "!rm -R results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrdJx7PTDqHS"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBpERO4iDbAr"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-kHYRA1NG1U"
      },
      "source": [
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy9N8GEkxxWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b3092c8-4dad-4d23-dad5-ed8050344df1"
      },
      "source": [
        "def _mp_fn(index):\n",
        "    device = xm.xla_device()\n",
        "    # We wrap this \n",
        "    model = WRAPPED_MODEL.to(device)\n",
        "\n",
        "    train_nli(model)\n",
        "\n",
        "xmp.spawn(_mp_fn, start_method=\"fork\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 209128\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 9804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:27:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.381700</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.299500</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.268800</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.289300</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.257900</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.251900</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.231500</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.218100</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.230100</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.234000</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.212800</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.217600</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.207400</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.191500</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.190900</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.195200</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.182300</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:27:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.378500</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.309700</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.274300</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.273100</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.274200</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.244900</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.249600</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.212100</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.221800</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.225400</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.226000</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.235100</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.215600</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.186300</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.179000</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.195400</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.184100</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.178400</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.176500</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:27:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.373200</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.308000</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.269600</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.271500</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.274200</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.250500</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.261900</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.261100</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.248600</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.226200</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.224800</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.230200</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.190800</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.173400</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.192000</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.186900</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.181500</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:27:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.384800</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.306300</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.264400</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.282000</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.285900</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.276000</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.251700</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.236600</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.233400</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.239000</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.214000</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.216400</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.212200</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.163900</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.212700</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.177400</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.189700</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.202200</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:27:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.377200</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.270800</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.313100</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.268600</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.260700</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.269600</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.245700</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.236800</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.210300</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.219900</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.226100</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.226100</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.207100</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.215200</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.192400</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.177100</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.196300</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.165800</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:27:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.372100</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.284600</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.275900</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.273500</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.277500</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.249300</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.239200</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.225900</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.231300</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.221100</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.233700</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.220100</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.227300</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.196500</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.184600</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.190400</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.183600</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.186500</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.192100</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:27:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.387400</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.302000</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.291600</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.275200</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.291100</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.282400</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.256600</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.230300</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.237800</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.202000</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.239900</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.220500</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.233300</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.222700</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.217000</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.198500</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.219200</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.165900</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.161600</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets... "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9804/9804 1:26:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.370500</td>\n",
              "      <td>0.836314</td>\n",
              "      <td>0.846075</td>\n",
              "      <td>0.838160</td>\n",
              "      <td>0.881682</td>\n",
              "      <td>0.833720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.268100</td>\n",
              "      <td>0.437596</td>\n",
              "      <td>0.885926</td>\n",
              "      <td>0.882960</td>\n",
              "      <td>0.899074</td>\n",
              "      <td>0.878815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.273600</td>\n",
              "      <td>0.350168</td>\n",
              "      <td>0.884360</td>\n",
              "      <td>0.881221</td>\n",
              "      <td>0.898508</td>\n",
              "      <td>0.876967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>0.367807</td>\n",
              "      <td>0.891613</td>\n",
              "      <td>0.889442</td>\n",
              "      <td>0.899639</td>\n",
              "      <td>0.886071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.273700</td>\n",
              "      <td>0.408614</td>\n",
              "      <td>0.894869</td>\n",
              "      <td>0.892944</td>\n",
              "      <td>0.901567</td>\n",
              "      <td>0.889836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.254900</td>\n",
              "      <td>0.369992</td>\n",
              "      <td>0.891737</td>\n",
              "      <td>0.889804</td>\n",
              "      <td>0.897949</td>\n",
              "      <td>0.886816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.251500</td>\n",
              "      <td>0.367093</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894769</td>\n",
              "      <td>0.901759</td>\n",
              "      <td>0.891994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.248700</td>\n",
              "      <td>0.436593</td>\n",
              "      <td>0.888687</td>\n",
              "      <td>0.886108</td>\n",
              "      <td>0.899356</td>\n",
              "      <td>0.882294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.246400</td>\n",
              "      <td>0.416772</td>\n",
              "      <td>0.893509</td>\n",
              "      <td>0.891789</td>\n",
              "      <td>0.898431</td>\n",
              "      <td>0.889114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.226100</td>\n",
              "      <td>0.476956</td>\n",
              "      <td>0.893592</td>\n",
              "      <td>0.891602</td>\n",
              "      <td>0.900558</td>\n",
              "      <td>0.888441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.217100</td>\n",
              "      <td>0.455888</td>\n",
              "      <td>0.894910</td>\n",
              "      <td>0.892731</td>\n",
              "      <td>0.903690</td>\n",
              "      <td>0.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.215600</td>\n",
              "      <td>0.469462</td>\n",
              "      <td>0.897136</td>\n",
              "      <td>0.895481</td>\n",
              "      <td>0.902118</td>\n",
              "      <td>0.892784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.192000</td>\n",
              "      <td>0.453030</td>\n",
              "      <td>0.895570</td>\n",
              "      <td>0.893462</td>\n",
              "      <td>0.903887</td>\n",
              "      <td>0.890013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.195300</td>\n",
              "      <td>0.517282</td>\n",
              "      <td>0.894581</td>\n",
              "      <td>0.892789</td>\n",
              "      <td>0.900200</td>\n",
              "      <td>0.889934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.195700</td>\n",
              "      <td>0.496484</td>\n",
              "      <td>0.896476</td>\n",
              "      <td>0.894624</td>\n",
              "      <td>0.902884</td>\n",
              "      <td>0.891576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.187700</td>\n",
              "      <td>0.463741</td>\n",
              "      <td>0.893756</td>\n",
              "      <td>0.891524</td>\n",
              "      <td>0.902724</td>\n",
              "      <td>0.887963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.175800</td>\n",
              "      <td>0.528649</td>\n",
              "      <td>0.892026</td>\n",
              "      <td>0.889789</td>\n",
              "      <td>0.900657</td>\n",
              "      <td>0.886300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.191800</td>\n",
              "      <td>0.486691</td>\n",
              "      <td>0.894086</td>\n",
              "      <td>0.892062</td>\n",
              "      <td>0.901423</td>\n",
              "      <td>0.888819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.189800</td>\n",
              "      <td>0.520109</td>\n",
              "      <td>0.892850</td>\n",
              "      <td>0.890566</td>\n",
              "      <td>0.902040</td>\n",
              "      <td>0.886971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-6500\n",
            "Configuration saved in ./results/checkpoint-6500/config.json\n",
            "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-7000\n",
            "Configuration saved in ./results/checkpoint-7000/config.json\n",
            "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-7500\n",
            "Configuration saved in ./results/checkpoint-7500/config.json\n",
            "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-8000\n",
            "Configuration saved in ./results/checkpoint-8000/config.json\n",
            "Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-8500\n",
            "Configuration saved in ./results/checkpoint-8500/config.json\n",
            "Model weights saved in ./results/checkpoint-8500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-9000\n",
            "Configuration saved in ./results/checkpoint-9000/config.json\n",
            "Model weights saved in ./results/checkpoint-9000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 24265\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-9500\n",
            "Configuration saved in ./results/checkpoint-9500/config.json\n",
            "Model weights saved in ./results/checkpoint-9500/pytorch_model.bin\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-6000 (score: 0.8954812138005916).\n",
            "Saving model checkpoint to nli_model/\n",
            "Configuration saved in nli_model/config.json\n",
            "Model weights saved in nli_model/pytorch_model.bin\n",
            "tokenizer config file saved in nli_model/tokenizer_config.json\n",
            "Special tokens file saved in nli_model/special_tokens_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehzjNVfL5QwS"
      },
      "source": [
        "load_model = XLNetForSequenceClassification.from_pretrained('/content/results/checkpoint-6000')\n",
        "load_model.train()\n",
        "\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(load_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duqtrX9lMtrX"
      },
      "source": [
        "def eval_nli(model, epochs=10, warmup_steps=200):\n",
        "    \"\"\"\n",
        "    This contains everything that must be done to train our models\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Loading datasets... \", end=\"\")\n",
        "    val_dataset = Formal_informal(val_encodings, val_labels)\n",
        "    test_dataset = Formal_informal(test_encodings, test_labels)\n",
        "        \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results_1',          # output directory\n",
        "        num_train_epochs=3,              # total number of training epochs\n",
        "        per_device_train_batch_size=8,  # batch size per device during training\n",
        "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "        weight_decay=0.01,               # strength of weight decay\n",
        "        logging_dir='./logs',            # directory for storing logs\n",
        "        logging_steps=5,\n",
        "        evaluation_strategy = 'steps',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "        args=training_args,                  # training arguments, defined above\n",
        "        train_dataset=val_dataset,         # training dataset\n",
        "        eval_dataset=test_dataset,             # evaluation dataset\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    trainer.place_model_on_device = False\n",
        "    trainer.train()\n",
        "\n",
        "    # trainer.save_model(\"nli_model/\")\n",
        "    # tokenizer.save_pretrained(\"nli_model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsf1hNyS_ZK1"
      },
      "source": [
        "train_dataset = Formal_informal(train_encodings, train_labels)\n",
        "val_dataset = Formal_informal(val_encodings, val_labels)\n",
        "test_dataset = Formal_informal(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxI9sCQz9G1M"
      },
      "source": [
        "root_path = '/content/drive/MyDrive/Colab Notebooks/Informal to formal/'\n",
        "name = 'xlnet_base_cased_895481'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnX3ZCBvSZ_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8036b73e-ae20-49c5-a179-7f2c8f3a5994"
      },
      "source": [
        "load_model.save_pretrained(name)\n",
        "tokenizer.save_pretrained(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('xlnet_base_cased_895481/tokenizer_config.json',\n",
              " 'xlnet_base_cased_895481/special_tokens_map.json',\n",
              " 'xlnet_base_cased_895481/spiece.model',\n",
              " 'xlnet_base_cased_895481/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuvrQ8lU8_Ca"
      },
      "source": [
        "!7z a {name} {'./' + name}\n",
        "shutil.copyfile(name + '.7z', root_path + name + '.7z')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBKcZxpK9LD6",
        "outputId": "4e15e24f-729b-4bbc-8b65-281c3b4d40d8"
      },
      "source": [
        "shutil.copyfile(root_path + name + '.7z', name + '.7z')\n",
        "!7z x {name + '.7z'}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 431179396 bytes (412 MiB)\n",
            "\n",
            "Extracting archive: xlnet_base_cased_887378.7z\n",
            "--\n",
            "Path = xlnet_base_cased_887378.7z\n",
            "Type = 7z\n",
            "Physical Size = 431179396\n",
            "Headers Size = 314\n",
            "Method = LZMA2:24\n",
            "Solid = +\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% 1 - xlnet_base_cased_895481/config.json\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 18% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 23% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 28% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 31% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 32% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 34% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 37% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 39% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 42% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 44% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 45% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 50% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 54% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 58% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 61% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 68% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 80% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 82% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 83% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 86% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 88% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 90% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 91% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 94% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 96% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 98% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 2 - xlnet_base_cased_895481/pytorch_model.bin\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 99% 3 - xlnet_base_cased_895481/special_tokens_map.json\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Folders: 1\n",
            "Files: 5\n",
            "Size:       470119310\n",
            "Compressed: 431179396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdWnTSWNgSBG"
      },
      "source": [
        "load_model = XLNetForSequenceClassification.from_pretrained(name)\n",
        "load_model.eval()\n",
        "tokenizer = XLNetTokenizer.from_pretrained(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8OIyNmS-KJH"
      },
      "source": [
        "load_model = DistilBertForSequenceClassification.from_pretrained(name)\n",
        "load_model.eval()\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFFaC5bNGqY7"
      },
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    num_workers=4,\n",
        "    drop_last=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMnld2kWKsvZ"
      },
      "source": [
        "valid_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=64,\n",
        "    num_workers=4,\n",
        "    drop_last=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8xzw3Dq_bK1"
      },
      "source": [
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    num_workers=4,\n",
        "    drop_last=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG6FyVktBOST"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n7micDXCteB"
      },
      "source": [
        "model = load_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeGTpkG6_IP9"
      },
      "source": [
        "list_predited_label = []\n",
        "list_label = []\n",
        "with torch.no_grad():\n",
        "    for d in test_dataloader:\n",
        "        input_ids = d[\"input_ids\"].to(device) # .reshape(64, 24)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        logits = outputs[0]\n",
        "\n",
        "        _, prediction = torch.max(logits, dim=1)\n",
        "        targets = d[\"labels\"].detach().numpy().tolist()\n",
        "        prediction = prediction.cpu().detach().numpy().tolist()\n",
        "\n",
        "        list_label.extend(targets)\n",
        "        list_predited_label.extend(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M20HKL8REsZF"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHTmwxcWWWNn"
      },
      "source": [
        "### ÐŸÑ€Ð¾Ñ‰Ðµ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vuaA8fwE2nA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "8ebe5315-3981-45ef-eb71-0f409c86ff75"
      },
      "source": [
        "confusion_mtx = confusion_matrix(list_label, list_predited_label, )\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_mtx.astype('float') / confusion_mtx.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm, annot=True, fmt='.2f')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvQcZIrmWchL"
      },
      "source": [
        "### classification_report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxi1TWoPE6uw",
        "outputId": "ffa61da6-3176-4a28-dd73-55dafb0ff451"
      },
      "source": [
        "result = classification_report(list_label, list_predited_label, digits=6)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0   0.820554  0.945465  0.878592     22151\n",
            "           1   0.924862  0.764512  0.837077     19449\n",
            "\n",
            "    accuracy                       0.860865     41600\n",
            "   macro avg   0.872708  0.854989  0.857835     41600\n",
            "weighted avg   0.869320  0.860865  0.859183     41600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}