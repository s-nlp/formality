{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training multilingual.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNeuwmda8WUbCQCSemPEDEv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7e9WU9zhnB5C","executionInfo":{"status":"ok","timestamp":1636968838071,"user_tz":480,"elapsed":6693,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}},"outputId":"beccce0d-2e42-4b17-bbae-a7d38a9cc466"},"source":["import os\n","from glob import glob\n","\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","from google.colab import drive\n","import os\n","import torch\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","\n","drive.mount('/content/drive')\n","root_path = '/content/drive/My Drive/Colab Notebooks/Sentiment Classifier/'"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28HnuyOxKzi3","executionInfo":{"status":"ok","timestamp":1636968839440,"user_tz":480,"elapsed":1385,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}},"outputId":"93acc609-0157-468d-d109-43a3dbe53c38"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"id":"tjuAI8LhJ_hQ","executionInfo":{"status":"ok","timestamp":1636968839441,"user_tz":480,"elapsed":7,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["dir_path = 'XFORMAL'\n","file_path = dir_path + '.zip'\n","if not os.path.isdir(dir_path):\n","    if not os.path.isfile(file_path):\n","        !wget -O XFORMAL.zip \"https://docs.google.com/uc?export=download&id=1cF8AXSQ1OZhgIbaIBWc3n8xgj2_prY8C\"\n","    !7z x XFORMAL.zip\n","\n","    for i in glob('XFORMAL/gyafc_translated/*/*/*/'):\n","        list_files = os.listdir(i)\n","        informal_path = os.path.join(i, '1_informal')\n","        os.mkdir(informal_path)\n","        formal_path = os.path.join(i, '1_formal')\n","        os.mkdir(formal_path)\n","        for j in list_files:\n","            if 'informal' in j:\n","                os.rename(\n","                    os.path.join(i, j),\n","                    os.path.join(informal_path, j),\n","                )\n","            else:\n","                os.rename(\n","                    os.path.join(i, j),\n","                    os.path.join(formal_path, j),\n","                )\n","        os.rename(informal_path, os.path.join(i, 'informal'))\n","        os.rename(formal_path, os.path.join(i, 'formal'))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xa2s0Ps7KbNM","executionInfo":{"status":"ok","timestamp":1636968839441,"user_tz":480,"elapsed":6,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["def get_label(file_path):\n","    parts = tf.strings.split(file_path, os.path.sep)\n","    # Note: You'll use indexing here instead of tuple unpacking to enable this \n","    # to work in a TensorFlow graph.\n","    return parts[-2]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zZCHFkNKdI-","executionInfo":{"status":"ok","timestamp":1636968839442,"user_tz":480,"elapsed":6,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["class_names = ['formal', 'informal']"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5584xpbKfLe","executionInfo":{"status":"ok","timestamp":1636968839442,"user_tz":480,"elapsed":6,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["def labeler(example, example_path):\n","    return example, tf.argmax(get_label(example_path) == class_names)\n","\n","def labeler(example_path):\n","    return get_label(example_path)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_qqETQ6Kgdt","executionInfo":{"status":"ok","timestamp":1636968839442,"user_tz":480,"elapsed":6,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["data_path = 'XFORMAL/gyafc_translated/*/*/{}/*/*'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CjrcCfVnjrHJ","executionInfo":{"status":"ok","timestamp":1636968839443,"user_tz":480,"elapsed":7,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["BATCH_SIZE = 512\n","BUFFER_SIZE = 200000"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqs6xGbFemWR","executionInfo":{"status":"ok","timestamp":1636968840514,"user_tz":480,"elapsed":1077,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["from transformers import BertTokenizer\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n","\n","def get_files(path_dataset,):\n","\n","\n","    data = {\"fr\" : {\"formal\" : [], \"informal\" : []},\n","            \"pt\" : {\"formal\" : [], \"informal\" : []},\n","            \"en\" : {\"formal\" : [], \"informal\" : []},\n","            \"it\" : {\"formal\" : [], \"informal\" : []},\n","            \"ru\" : {\"formal\" : [], \"informal\" : []}\n","            }\n","\n","    for file_name in glob(path_dataset):\n","      with open(file_name, \"r\") as f:\n","        content = f.readlines()\n","\n","        # print(f\"language: {file_name[25:27]}, type: {file_name}, lenght: {len(content)},  label: {get_label(file_name)}\")\n","        \n","\n","        data[file_name[25:27]][str(get_label(file_name).numpy())[2:-1]] += content\n","\n","    # data = {\n","    #         \"fr\" : {\"formal\": list(set(data[\"fr\"][\"formal\"])), \"informal\": list(set(data[\"fr\"][\"informal\"]))},\n","    #         \"pt\" : {\"formal\": list(set(data[\"pt\"][\"formal\"])), \"informal\": list(set(data[\"pt\"][\"informal\"]))},\n","    #         \"en\" : {\"formal\": list(set(data[\"en\"][\"formal\"])), \"informal\": list(set(data[\"en\"][\"informal\"]))},\n","    #         \"it\" : {\"formal\": list(set(data[\"it\"][\"formal\"])), \"informal\": list(set(data[\"it\"][\"informal\"]))},\n","    #         \"ru\" : {\"formal\": list(set(data[\"ru\"][\"formal\"])), \"informal\": list(set(data[\"ru\"][\"informal\"]))}\n","    #         }\n","\n","    # data = {\n","    #         \"fr\" : {\"formal\": [sentence for sentence in list(set(data[\"fr\"][\"formal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50],\n","    #                 \"informal\": [sentence for sentence in list(set(data[\"fr\"][\"informal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50]},\n","            \n","    #         \"pt\" : {\"formal\": [sentence for sentence in list(set(data[\"pt\"][\"formal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50],\n","    #                 \"informal\": [sentence for sentence in list(set(data[\"pt\"][\"informal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50]},\n","            \n","    #         \"en\" : {\"formal\": [sentence for sentence in list(set(data[\"en\"][\"formal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50],\n","    #                 \"informal\": [sentence for sentence in list(set(data[\"en\"][\"informal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50]},\n","            \n","    #         \"it\" : {\"formal\": [sentence for sentence in list(set(data[\"it\"][\"formal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50],\n","    #                 \"informal\": [sentence for sentence in list(set(data[\"it\"][\"informal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50]},\n","            \n","    #         \"ru\" : {\"formal\": [sentence for sentence in list(set(data[\"ru\"][\"formal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50],\n","    #                 \"informal\": [sentence for sentence in list(set(data[\"ru\"][\"informal\"])) if list(bert_tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"].shape)[1]<=50]},\n","    #         }\n","\n","    data = {\n","            \"fr\" : {\"formal\": [sentence for sentence in list(set(data[\"fr\"][\"formal\"])) if len(sentence) <=150],\n","                    \"informal\": [sentence for sentence in list(set(data[\"fr\"][\"informal\"])) if len(sentence) <=150]},\n","            \n","            \"pt\" : {\"formal\": [sentence for sentence in list(set(data[\"pt\"][\"formal\"])) if len(sentence) <=150],\n","                    \"informal\": [sentence for sentence in list(set(data[\"pt\"][\"informal\"])) if len(sentence) <=150]},\n","            \n","            \"en\" : {\"formal\": [sentence for sentence in list(set(data[\"en\"][\"formal\"])) if len(sentence) <=150],\n","                    \"informal\": [sentence for sentence in list(set(data[\"en\"][\"informal\"])) if len(sentence) <=150]},\n","            \n","            \"it\" : {\"formal\": [sentence for sentence in list(set(data[\"it\"][\"formal\"])) if len(sentence) <=150],\n","                    \"informal\": [sentence for sentence in list(set(data[\"it\"][\"informal\"])) if len(sentence) <=150]}, \n","            # \"ru\" : {\"formal\": [sentence for sentence in list(set(data[\"ru\"][\"formal\"])) if len(sentence) <=150],\n","            #         \"informal\": [sentence for sentence in list(set(data[\"ru\"][\"informal\"])) if len(sentence) <=150]}, \n","            }\n","\n","            \n","\n","    return data\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y517vbTbeoG1","executionInfo":{"status":"ok","timestamp":1636968843129,"user_tz":480,"elapsed":2617,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}},"outputId":"a20383d1-b026-4ffc-bdcb-85c12f3d9536"},"source":["\n","\n","train_data  = get_files(data_path.format('train'))\n","\n","for language in train_data:\n","  print(f\"language: {language}, # formal: {len(train_data[language]['formal'])}, # informal: {len(train_data[language]['informal'])}\")\n","print('---------------------------')\n","\n","validation_data = get_files(data_path.format('test'))\n","for language in validation_data:\n","  print(f\"language: {language}, # formal: {len(validation_data[language]['formal'])}, # informal: {len(validation_data[language]['informal'])}\")\n","print('---------------------------')\n","\n","test_data = get_files(data_path.format('tune'))\n","for language in train_data:\n","  print(f\"language: {language}, # formal: {len(test_data[language]['formal'])}, # informal: {len(test_data[language]['informal'])}\")\n","\n","all_data = [train_data, validation_data, test_data]\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["language: fr, # formal: 103209, # informal: 100173\n","language: pt, # formal: 103630, # informal: 100260\n","language: en, # formal: 103979, # informal: 100386\n","language: it, # formal: 103536, # informal: 100236\n","---------------------------\n","language: fr, # formal: 10326, # informal: 9016\n","language: pt, # formal: 10418, # informal: 9016\n","language: en, # formal: 10710, # informal: 9031\n","language: it, # formal: 10402, # informal: 9001\n","---------------------------\n","language: fr, # formal: 21431, # informal: 19354\n","language: pt, # formal: 21602, # informal: 19363\n","language: en, # formal: 22132, # informal: 19448\n","language: it, # formal: 21483, # informal: 19349\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rn4D1AdZOKzX","executionInfo":{"status":"ok","timestamp":1636968843129,"user_tz":480,"elapsed":5,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}},"outputId":"90243f01-4f39-473e-c4a2-f25623a7e2e3"},"source":["from random import shuffle\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","class SentenceDataset(Dataset):\n","    def __init__(self, datasets):\n","      self.formal = []\n","      self.informal = []\n","      if isinstance(datasets, list):\n","        for dataset in datasets:\n","            self.formal += [sentence for sentence in dataset['formal']]\n","            self.informal += [sentence for sentence in dataset['informal']]  \n","      else:\n","        for language in datasets:\n","          self.formal += [sentence for sentence in datasets[language]['formal']]\n","          self.informal += [sentence for sentence in datasets[language]['informal']]        \n","\n","      print(f\"formal sentences: {len(self.formal)}\")\n","      print(f\"informal sentences: {len(self.informal)}\")\n","\n","      self.tuples = []\n","      self.tuples += [(sentence,0) for sentence in self.formal] \n","      self.tuples += [(sentence,1) for sentence in self.informal]\n","      shuffle(self.tuples)\n","\n","    def __len__(self):\n","        return len(self.tuples)\n","\n","    def __getitem__(self, idx):\n","        return self.tuples[idx]"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yiUzGFhMPAIO","executionInfo":{"status":"ok","timestamp":1636968906652,"user_tz":480,"elapsed":63526,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}},"outputId":"ce821a53-2007-47e4-bb51-c6bc51442f46"},"source":["from torch import nn\n","class char_BiLSTM(nn.Module):\n","    def __init__(self, embedding_dim=32, hidden_dim=128, lstm_layer=1, output=2, train = True):\n","        \n","        super(char_BiLSTM, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        \n","        # load pre-trained embeddings\n","        self.embedding = nn.Embedding(len(char_vocab)+1, embedding_dim)\n","        # embeddings are not fine-tuned\n","        self.embedding.weight.requires_grad = train\n","        \n","        # RNN layer with LSTM cells\n","        # OR self.lstm = NaiveLSTM(input_sz = self.embedding.embedding_dim, hidden_sz = hidden_dim)\n","        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n","                            hidden_size=hidden_dim,\n","                            num_layers=lstm_layer, \n","                            bidirectional=True,\n","                            dropout = 0.5)\n","        # dense layer\n","        self.output = nn.Linear(hidden_dim*2, output)\n","    \n","    def forward(self, sents):\n","        x = self.embedding(sents[0])\n","        \n","        # the original dimensions of torch LSTM's output are: (seq_len, batch, num_directions * hidden_size)\n","        lstm_out, _ = self.lstm(x)\n","        \n","        # reshape to get the tensor of dimensions (seq_len, batch, num_directions, hidden_size)\n","        lstm_out = lstm_out.view(x.shape[0], -1, 2, self.hidden_dim)#.squeeze(1)\n","        \n","\n","        # we take the last hidden state of the forward LSTM and the first hidden state of the backward LSTM\n","        dense_input = torch.cat((lstm_out[-1, :, 0, :], lstm_out[0, :, 1, :]), dim=1)\n","        \n","        y = self.output(dense_input)#.view([1, 2])\n","        return [y]\n","\n","text = []\n","for set_data in all_data:\n","  for language in set_data:\n","    text += set_data[language]['formal'] \n","    text += set_data[language]['informal'] \n","text = [sentence for sentence in text if len(sentence) <= 150]\n","\n","print(f\"total sentences: {len(text)}\")\n","\n","MAX_LEN = max([len(sentence) for sentence in text])\n","PRUNE_TOKENS_LESS_THAN = 0\n","\n","print(\"maxlen :\", str(MAX_LEN))\n","\n","\n","tokens = set.union(*[set(sentence) for sentence in text])\n","token_stats = {token :  0 for token in tokens}\n","text = ' '.join(text)\n","for token in list(tokens):\n","  token_stats[token] = text.count(token)\n","\n","\n","tokens = [key for (key,value) in token_stats.items() if value >= PRUNE_TOKENS_LESS_THAN]\n","tokens = sorted(tokens)\n","tokens = {key:id for (id,key) in enumerate(tokens)}\n","tokens[\"<EMP>\"] = len(tokens)\n","tokens[\"<UNK>\"] = len(tokens)\n","print(f\"total tokens: {len(list(tokens))}\")\n","\n","def char_pipeline(text, length):\n","  chars = [char for char in text]\n","  text = char_vocab(chars)\n","  text += [char_vocab[\"<EMP>\"]] * (length - len(text))\n","  return torch.tensor(text, dtype=torch.int64)\n","\n","\n","def char_collate_batch(batch):\n","  max_length = len(max(batch, key=lambda t: len(t[0]))[0])\n","  label_list, text_list = [], []\n","  for (_text, _label) in batch:\n","        label_list.append(_label)\n","        processed_text = char_pipeline(_text, max_length)\n","        text_list.append(processed_text)\n","  label_list = torch.tensor(label_list, dtype=torch.int64)\n","  text_list = torch.stack(text_list, dim=-1)\n","  # print(\"Text List \",list(text_list.size()))\n","  # print(\"Label List \",list(label_list.size()))\n","  return label_list.to(device), text_list.to(device)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["total sentences: 1057491\n","maxlen : 150\n","total tokens: 218\n"]}]},{"cell_type":"code","metadata":{"id":"KS2HqG9fkIKb","executionInfo":{"status":"ok","timestamp":1636968906652,"user_tz":480,"elapsed":11,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["\n","def char_pipeline(text, length):\n","  chars = [char for char in text]\n","  text = char_vocab(chars)\n","  text += [char_vocab[\"<EMP>\"]] * (length - len(text))\n","  return torch.tensor(text, dtype=torch.int64)\n","\n","\n","def char_collate_batch(batch):\n","  max_length = len(max(batch, key=lambda t: len(t[0]))[0])\n","  label_list, text_list = [], []\n","  for (_text, _label) in batch:\n","        label_list.append(_label)\n","        processed_text = char_pipeline(_text, max_length)\n","        text_list.append(processed_text)\n","  label_list = torch.tensor(label_list, dtype=torch.int64)\n","  text_list = torch.stack(text_list, dim=-1)\n","  # print(\"Text List \",list(text_list.size()))\n","  # print(\"Label List \",list(label_list.size()))\n","  return [text_list], [label_list]"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVS6XVkCUGas","executionInfo":{"status":"ok","timestamp":1636968906653,"user_tz":480,"elapsed":11,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["from torchtext.vocab import vocab\n","\n","char_vocab = vocab(tokens)\n","char_vocab.set_default_index(len(tokens)-1)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjvrNPetddfL","executionInfo":{"status":"ok","timestamp":1636968906653,"user_tz":480,"elapsed":11,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["# !wget http://nlp.stanford.edu/data/glove.6B.zip\n","# !unzip glove.6B.zip"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"muPyeQOXOMy0","executionInfo":{"status":"ok","timestamp":1636968906653,"user_tz":480,"elapsed":10,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["#@title Word BiLSTM model\n","\n","# from torchtext.data.utils import get_tokenizer\n","# from torchtext.vocab import build_vocab_from_iterator\n","# from nltk import wordpunct_tokenize\n","\n","\n","# class word_BiLSTM(nn.Module):\n","#     def __init__(self, embeddings, trainable = False, hidden_dim=128, lstm_layer=1, output=2):\n","        \n","#         super(word_BiLSTM, self).__init__()\n","#         self.hidden_dim = hidden_dim\n","        \n","#         # load pre-trained embeddings\n","#         self.embedding = nn.Embedding.from_pretrained(embeddings)\n","#         # embeddings are not fine-tuned\n","#         self.embedding.weight.requires_grad = trainable\n","        \n","#         # RNN layer with LSTM cells\n","#         # OR self.lstm = NaiveLSTM(input_sz = self.embedding.embedding_dim, hidden_sz = hidden_dim)\n","#         self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n","#                             hidden_size=hidden_dim,\n","#                             num_layers=lstm_layer, \n","#                             bidirectional=True)\n","#         # dense layer\n","#         self.output = nn.Linear(hidden_dim*2, output)\n","    \n","#     def forward(self, sents):\n","#         x = self.embedding(sents)\n","        \n","#         # the original dimensions of torch LSTM's output are: (seq_len, batch, num_directions * hidden_size)\n","#         lstm_out, _ = self.lstm(x)\n","        \n","#         # reshape to get the tensor of dimensions (seq_len, batch, num_directions, hidden_size)\n","#         lstm_out = lstm_out.view(x.shape[0], -1, 2, self.hidden_dim)#.squeeze(1)\n","        \n","#         # lstm_out[:, :, 0, :] -- output of the forward LSTM\n","#         # lstm_out[:, :, 1, :] -- output of the backward LSTM\n","#         # we take the last hidden state of the forward LSTM and the first hidden state of the backward LSTM\n","#         dense_input = torch.cat((lstm_out[-1, :, 0, :], lstm_out[0, :, 1, :]), dim=1)\n","        \n","#         y = self.output(dense_input)#.view([1, 2])\n","#         return y\n","\n","\n","\n","# text = formal + informal\n","# word_vocab = set()\n","# for sentence in text:\n","#   words = wordpunct_tokenize(sentence)\n","#   word_vocab.update(words)\n","\n","# def load_embeddings(emb_path, vocab):\n","#     clf_embeddings = {}\n","#     emb_vocab = set()\n","#     for line in open(emb_path):\n","#         line = line.strip('\\n').split()\n","#         word, emb = line[0], line[1:]\n","#         emb = [float(e) for e in emb]\n","#         if word in vocab:\n","#             clf_embeddings[word] = emb\n","#     for w in vocab:\n","#         if w in clf_embeddings:\n","#             emb_vocab.add(w)\n","#     word2idx = {w: idx for (idx, w) in enumerate(emb_vocab)}\n","#     max_val = max(word2idx.values())\n","    \n","#     word2idx['UNK'] = max_val + 1\n","#     word2idx['EOS'] = max_val + 2\n","#     emb_dim = len(list(clf_embeddings.values())[0])\n","#     clf_embeddings['UNK'] = [0.0 for i in range(emb_dim)]\n","#     clf_embeddings['EOS'] = [0.0 for i in range(emb_dim)]\n","    \n","#     embeddings = [[] for i in range(len(word2idx))]\n","#     for w in word2idx:\n","#         embeddings[word2idx[w]] = clf_embeddings[w]\n","#     embeddings = torch.Tensor(embeddings)\n","#     return embeddings, word2idx\n","\n","# word_embeddings, word_vocab = load_embeddings('glove.6B.300d.txt', word_vocab)    \n","\n","# def text_pipeline_words(text, length):\n","#   chars = wordpunct_tokenize(text)  \n","#   text = [word_vocab.get(char, word_vocab['UNK']) for char in chars]\n","#   text += [word_vocab[\"EOS\"]] * (length - len(text))\n","#   # print(torch.tensor(text, dtype=torch.int64))\n","#   return torch.tensor(text, dtype=torch.int64)\n","\n","# def collate_batch_words(batch):\n","#   sentences = [text for (text, label) in batch]\n","#   max_length = 0\n","\n","#   for sentence in sentences:\n","#     if len(wordpunct_tokenize(sentence)) > max_length:\n","#       max_length = len(wordpunct_tokenize(sentence))\n","\n","#   label_list, text_list = [], []\n","#   for (_text, _label) in batch:\n","#         label_list.append(_label)\n","#         processed_text = text_pipeline_words(_text, max_length)\n","#         text_list.append(processed_text)\n","#   label_list = torch.tensor(label_list, dtype=torch.int64)\n","#   text_list = torch.stack(text_list, dim=-1)\n","#   return [text_list], [label_list]\n","\n"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"EULg_5bMfH34","executionInfo":{"status":"ok","timestamp":1636968909989,"user_tz":480,"elapsed":3346,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["#@title BERT model\n","\n","from transformers import DistilBertModel, DistilBertTokenizer\n","from transformers import BertTokenizer, BertModel\n","from transformers import RobertaModel, RobertaTokenizer\n","\n","class BERT_Model(nn.Module):\n","    def __init__(self,bert_model, layer2, layer3, Train = False):\n","        super(BERT_Model, self).__init__()\n","\n","        self.bert = bert_model\n","        for param in self.bert.parameters():\n","            param.requires_grad = Train\n","\n","        \n","\n","        self.out = nn.Sequential(\n","          nn.Linear(self.bert.config.hidden_size, layer2),\n","          nn.Linear(layer2, layer3),\n","          nn.Linear(layer3, 2)\n","        )\n","\n","    def forward(self, input):\n","        id = input[0]\n","        mask = input[1]\n","\n","        bert_output = self.bert(id, attention_mask = mask)\n","        output = self.out(bert_output[1])\n","        return [output]\n","\n","class Distil_BERT_Model(nn.Module):\n","    def __init__(self, layer2, layer3, Train = False):\n","        super(Distil_BERT_Model, self).__init__()\n","\n","        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","        for param in self.bert.parameters():\n","            param.requires_grad = Train\n","        \n","        self.out = nn.Sequential(\n","          nn.Linear(self.bert.config.hidden_size, layer2),\n","          nn.Linear(layer2, layer3),\n","          nn.Linear(layer3, 2)\n","        )\n","    def forward(self, input):\n","        id = input[0]\n","        mask = input[1]\n","\n","        bert_output = self.bert(id, attention_mask = mask)\n","\n","        #take the last value from the bert output\n","        output = self.out(torch.flatten(bert_output[0].narrow(1,-1,1),1))\n","        return [output]\n","\n","class RoBERTa_Model(nn.Module):\n","    def __init__(self, layer2, layer3, Train = False):\n","        super(RoBERTa_Model, self).__init__()\n","\n","        self.bert = RobertaModel.from_pretrained('roberta-base')\n","        for param in self.bert.parameters():\n","            param.requires_grad = Train\n","        \n","        self.out = nn.Sequential(\n","          nn.Linear(self.bert.config.hidden_size, layer2),\n","          nn.Linear(layer2, layer3),\n","          nn.Linear(layer3, 2)\n","        )\n","\n","    def forward(self, input):\n","        id = input[0]\n","        mask = input[1]\n","\n","        bert_output = self.bert(id, attention_mask = mask)\n","\n","        #take the last value from the bert output\n","        output = self.out(torch.flatten(bert_output[0].narrow(1,-1,1),1))\n","        return [output]\n","\n","bert_cased_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","bert_uncased_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n","\n","Disitl_bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","RoBERTa_tokeniser = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","\n","def Distil_BERT_collate_batch(batch):\n","  sentences = [text for (text, label) in batch]\n","  labels = [label for (text, label) in batch]\n","\n","  # for sentence in sentences:\n","  temp = Disitl_bert_tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n","  text_list = temp[\"input_ids\"]\n","  mask_list = temp[\"attention_mask\"]\n","  label_list = torch.tensor(labels, dtype=torch.int64)\n","\n","  # text_list = torch.stack(text_list, dim=0)\n","  return  [text_list, mask_list], [label_list]\n","\n","def BERT_cased_collate_batch(batch):\n","  sentences = [text for (text, label) in batch]\n","  labels = [label for (text, label) in batch]\n","\n","  # for sentence in sentences:\n","  temp = bert_cased_tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n","  text_list = temp[\"input_ids\"]\n","  mask_list = temp[\"attention_mask\"]\n","  label_list = torch.tensor(labels, dtype=torch.int64)\n","\n","  # text_list = torch.stack(text_list, dim=0)\n","  return  [text_list, mask_list], [label_list]\n","\n","def BERT_uncased_collate_batch(batch):\n","  sentences = [text for (text, label) in batch]\n","  labels = [label for (text, label) in batch]\n","\n","  # for sentence in sentences:\n","  temp = bert_uncased_tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n","  text_list = temp[\"input_ids\"]\n","  mask_list = temp[\"attention_mask\"]\n","  label_list = torch.tensor(labels, dtype=torch.int64)\n","\n","  # text_list = torch.stack(text_list, dim=0)\n","  return  [text_list, mask_list], [label_list]\n","\n","def RoBERTa_collate_batch(batch):\n","  sentences = [text for (text, label) in batch]\n","  labels = [label for (text, label) in batch]\n","\n","  # for sentence in sentences:\n","  temp = RoBERTa_tokeniser(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n","  text_list = temp[\"input_ids\"]\n","  mask_list = temp[\"attention_mask\"]\n","  label_list = torch.tensor(labels, dtype=torch.int64)\n","\n","  # text_list = torch.stack(text_list, dim=0)\n","  return  [text_list, mask_list], [label_list]\n"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZCUsRVFUOPV","executionInfo":{"status":"ok","timestamp":1636968909990,"user_tz":480,"elapsed":3,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}}},"source":["#@title Train Methods\n","\n","import gc\n","def cleanup():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","def train(dataloader, log_interval):\n","    model.train()\n","    final_loss, accuracy, count = 0, 0, 0\n","    interval_time = time.time()\n","    epoch_time = interval_time\n","    for idx, (inputs, outputs) in enumerate(dataloader):\n","      cleanup()\n","      optimizer.zero_grad()\n","\n","      inputs = [input.to(device) for input in inputs]\n","      outputs = [output.to(device) for output in outputs]\n","      predicted_outputs = model(inputs)\n","      for output, predicted_output in zip(outputs, predicted_outputs):\n","        loss = criterion(predicted_output, output)\n","        loss.backward()\n","        final_loss += loss.item()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","      optimizer.step()\n","\n","      accuracy += (predicted_outputs[0].argmax(1) == outputs[0]).sum().item()\n","      count += outputs[0].size(0)\n","\n","      if idx % log_interval == 0 and idx > 0:\n","        elapsed = time.time() - interval_time\n","        print(f'| epoch {epoch} \\| {idx}/{len(dataloader)} batches \\| accuracy {accuracy/count} \\| time elapsed {elapsed} \\| batches/second {log_interval/elapsed}')\n","        interval_time = time.time()\n","      \n","\n","\n","    cleanup()\n","    return  final_loss/count, accuracy/count, time.time() -epoch_time\n","\n","\n","\n","def evaluate(dataloader):\n","  model.eval()\n","  accuracy, loss, count = 0,0,0\n","  with torch.no_grad():\n","    for idx, (inputs, outputs) in enumerate(dataloader):\n","      cleanup()\n","\n","      inputs = [input.to(device) for input in inputs]\n","      outputs = [output.to(device) for output in outputs]\n","\n","      predicted_outputs = model(inputs)\n","\n","      for output, predicted_output in zip(outputs, predicted_outputs):\n","        loss += criterion(predicted_output, output)\n","\n","      accuracy += (predicted_outputs[0].argmax(1) == outputs[0]).sum().item()\n","      count += outputs[0].size(0)\n","\n","      cleanup()\n","  return loss/count, accuracy/count\n","\n","\n","def get_labels(dataloader):\n","    model.eval()\n","    err_num = 0\n","    labels = []\n","    predicted_labels =[]\n","    with torch.no_grad():\n","      for idx, (inputs, outputs) in enumerate(dataloader): \n","\n","        cleanup()\n","\n","        inputs = [input.to(device) for input in inputs]\n","        outputs = [output.to(device) for output in outputs]\n","\n","        predicted_label = model(inputs)\n","        predicted_label = torch.argmax(predicted_label[0], 1)\n","        predicted_label = predicted_label.cpu().detach().numpy().tolist()\n","        label = outputs[0].cpu().detach().numpy()\n","\n","        predicted_labels.extend(predicted_label)\n","        labels.extend(label)\n","\n","        cleanup()\n","    return labels, predicted_labels"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjsrVpWqUSdz","executionInfo":{"status":"ok","timestamp":1636968913117,"user_tz":480,"elapsed":3129,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}},"outputId":"87ccc6b7-3998-4b76-f71c-15c39124d241"},"source":["#@title Logs\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","!pip install openpyxl\n","\n","import openpyxl as opx\n","from math import floor\n","\n","def number_to_column(num):\n","  if num < 26:\n","    return chr(num+65)\n","  else:\n","    return chr(floor(num/26)+64) + chr((num%26)+65)\n","\n","class logs():\n","  def __init__(self, folder, model_name, columns):\n","    wb = opx.load_workbook(filename = folder + \"/logs_multilingual.xlsx\")\n","    sheet_columns = {}\n","    if model_name in wb.sheetnames:\n","      print(f\"Found a sheet for the model {model_name}\")\n","      sheet = wb[model_name]\n","      idx = 1\n","      while sheet.cell(row = 1, column = idx).value:\n","        print(sheet.cell(row = 1, column = idx).value)\n","        sheet_columns[sheet.cell(row = 1, column = idx).value] = idx\n","        idx += 1\n","      print(sheet_columns)\n","      self.check_columns(columns, sheet_columns)\n","    else:\n","      self.check_columns(columns, sheet_columns)      \n","      print(f\"worksheet for {model_name} not found, creating new worksheet\")\n","      sheet = wb.create_sheet(model_name)\n","      for idx, column in enumerate(columns):\n","        sheet[f\"{number_to_column(idx)}1\"] = column\n","      wb.save(filename = folder + \"/logs_multilingual.xlsx\")\n","    if sheet_columns == {}:\n","      for idx, column in enumerate(columns):\n","        sheet_columns[\"colums\"] = idx + 1\n","    \n","    self.model_name = model_name\n","    self.sheet_columns = sheet_columns\n","    self.folder = folder\n","    \n","    self.train_acc = []\n","    self.val_acc = []\n","    self.train_loss = []\n","    self.val_loss = []\n","\n","    self.test_acc = 0\n","\n","  def check_columns(self, columns, sheet_columns):\n","    required_columns = [\"Epochs\", \"Batch size\",  \"Description\", \"epoch time\"]\n","    if sheet_columns != {}:\n","      sheet_columns = list(sheet_columns.keys())\n","      if sheet_columns  != columns:\n","        print(sheet_columns)\n","        print(\"---------\")\n","        print(columns)\n","        raise NameError(\"The provided columns do not align with columns in the sheet\")\n","    if not set(required_columns).issubset(columns):\n","      raise NameError(\"The provided columns lack some of the required columns\")\n","\n","  def reserve_id(self):\n","    wb = opx.load_workbook(filename = self.folder + \"/logs_multilingual.xlsx\")\n","    sheet = wb[self.model_name]\n","    idx = 1\n","    column = self.sheet_columns[\"Description\"]\n","    while sheet.cell(row = idx, column = column).value:\n","      idx += 1\n","    sheet[f\"{chr(column+64)}{idx}\"] = \"reserved\"\n","    wb.save(filename = self.folder + \"/logs_multilingual.xlsx\")    \n","\n","    self.model_folder = self.folder + f\"/data_logs/{self.model_name}#{idx}\"\n","\n","    return idx\n","\n","  def write_data(self, data, idx):\n","    wb = opx.load_workbook(filename = self.folder + \"/logs_multilingual.xlsx\")\n","    sheet = wb[self.model_name]\n","    for (row_name, data_point) in data:\n","      sheet[f\"{number_to_column(self.sheet_columns[row_name]-1)}{idx}\"] = data_point\n","    wb.save(filename = self.folder + \"/logs_multilingual.xlsx\")    \n","\n","\n","    # self.logdir = self.folder + f\"/data_logs/{self.model_name}#{self.}\"\n","    tensorboard_writer = SummaryWriter(self.model_folder)       \n","\n","    for epoch, val in enumerate(self.train_acc):\n","      tensorboard_writer.add_scalar('Accuracy/train', val, epoch)    \n","\n","    for epoch, val in enumerate(self.val_acc):\n","      tensorboard_writer.add_scalar('Accuracy/validation', val, epoch)    \n","\n","    for epoch, val in enumerate(self.train_loss):\n","      tensorboard_writer.add_scalar('Loss/train', val, epoch)    \n","\n","    for epoch, val in enumerate(self.val_loss):\n","      tensorboard_writer.add_scalar('Loss/validation', val, epoch)\n","\n","    tensorboard_writer.flush()\n","    tensorboard_writer.close()          "],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n","Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMwS0bhO9JgD","executionInfo":{"status":"ok","timestamp":1636968913118,"user_tz":480,"elapsed":13,"user":{"displayName":"Ivan Trifonov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00941829718680452139"}},"outputId":"3d4d2ff9-f907-4fb9-bd53-119db493fabd"},"source":["test = [train_data[temp] for temp in train_data.keys()]\n","print([train_data[temp].keys() for temp in train_data.keys()])\n"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["[dict_keys(['formal', 'informal']), dict_keys(['formal', 'informal']), dict_keys(['formal', 'informal']), dict_keys(['formal', 'informal'])]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ieUx7GzkU8ob","outputId":"b6cabf4f-3c6d-4139-e41f-0c78305cd1ad"},"source":["from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","import csv\n","cleanup()\n","\n","\n","# Hyperparameters\n","data = {}\n","EPOCHS = 10  # epoch\n","LR = 5  # learning rate\n","BATCH_SIZE = 512 # batch size for training\n","MAX_TRAIN_STEPS = 1\n","LOG_INTERVAL = 200\n","\n","NAME = \"char_BiLSTM\"\n","DESCRIPTION = \"multilingual char BiLSTM\"\n","EMBEDDING = 128\n","HIDDEN_STATE = 128\n","LAYERS = 2\n","\n","DROPOUT = 0.5\n","languages = ['fr','pt','it','all']\n","\n","\n","architectures = {}\n","# architectures[\"char_BiLSTM\"] = {\n","#     \"Folder\" : \"folder\",\n","#     \"Description\" : \"Char BiLSTM model\",\n","#     \"model\" : char_BiLSTM(embedding_dim=128, hidden_dim=128, lstm_layer=2, output=2, train = True).to(device),\n","#     \"collate_batch\" : char_collate_batch\n","# }    \n","architectures[\"BERT1\"] = {\n","    \"Folder\" : \"folder\",\n","    \"Description\" : \"Cased Multilingual BERT model\",\n","    # \"model\" : BERT_Model(BertModel.from_pretrained('bert-base-multilingual-cased'), 1024, 1024, Train = False).to(device),\n","    \"collate_batch\" : BERT_cased_collate_batch\n","}    \n","# architectures[\"BERT2\"] = {\n","#     \"Folder\" : \"folder\",   \n","#     \"Description\" : \"Uncased Multilingual BERT model\",\n","#     \"model\" : BERT_Model(BertModel.from_pretrained('bert-base-multilingual-uncased'), 1024, 1024, Train = False).to(device),\n","#     \"collate_batch\" : BERT_uncased_collate_batch\n","# }   \n","  \n","\n","\n","for architecture in architectures.keys():\n","  #--------------------\n","  #Create Logs\n","  #--------------------\n","  data[\"Name\"] = \"temp\"\n","  data[\"Epochs\"] = EPOCHS\n","  data[\"Batch size\"] = BATCH_SIZE\n","  # data[\"Embeddings Trainable\"] = \"True\"\n","  # data[\"Layers\"] = LAYERS\n","  # data[\"hidden state\"] = HIDDEN_STATE\n","\n","\n","  data[\"epoch time\"] = \"temp\"\n","  data[\"Description\"] = architectures[architecture][\"Description\"]\n","\n","  for language in languages:\n","    data[f\"{language}-Loss\"] = \"temp\"\n","    \n","    data[f\"{language}-train accuracy\"] = \"temp\"\n","    data[f\"{language}-validation accuracy\"] = \"temp\"\n","    data[f\"{language}-test accuracy\"] = \"temp\"\n","\n","    data[f'{language}-formal-precision'] = \"temp\"\n","    data[f'{language}-formal-recall'] = \"temp\"\n","    data[f'{language}-formal-f1-score'] = \"temp\"\n","\n","    data[f'{language}-informal-precision'] = \"temp\"\n","    data[f'{language}-informal-recall'] = \"temp\"\n","    data[f'{language}-informal-f1-score'] = \"temp\"\n","\n","\n","  log = logs(\"/content/drive/My Drive/Colab Notebooks/Sentiment Classifier\", architectures[architecture]['Folder'], list(data.keys()))\n","  id = log.reserve_id()\n","\n","  try: \n","      os.mkdir(log.model_folder) \n","  except OSError as error: \n","      print(error)  \n","  for language in languages:\n","    if language == \"all\":\n","      train_dataset  = SentenceDataset([train_data[temp] for temp in train_data.keys()])\n","      val_dataset  = SentenceDataset([validation_data[temp] for temp in validation_data.keys()])\n","      test_dataset  = SentenceDataset([test_data[temp] for temp in test_data.keys()])\n","    else:\n","      train_dataset  = SentenceDataset({language:train_data[language]})\n","      val_dataset  = SentenceDataset({language:validation_data[language]})\n","      test_dataset  = SentenceDataset({language:test_data[language]})\n","\n","\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=architectures[architecture]['collate_batch'], num_workers=4)\n","    valid_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=architectures[architecture]['collate_batch'], num_workers=4)\n","    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=architectures[architecture]['collate_batch'], num_workers=4)\n","\n","\n","    model = BERT_Model(BertModel.from_pretrained('bert-base-multilingual-cased'), 1024, 1024, Train = False).to(device)\n","\n","\n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.8)\n","\n","    total_accu = None      \n","\n","    epoch_train_accuracy = []\n","    epoch_val_accuracy = []\n","    epoch_time_list = []\n","\n","    for epoch in range(0, EPOCHS):\n","\n","\n","\n","      epoch_start_time = time.time()\n","      epoch_loss, epoch_acc, epoch_time = train(train_dataloader, LOG_INTERVAL)\n","      validation_loss, validation_acc = evaluate(valid_dataloader)\n","\n","\n","\n","      if epoch >= MAX_TRAIN_STEPS:\n","        STEP_SCHEDULER = True\n","        for i in range(MAX_TRAIN_STEPS):\n","          if epoch_val_accuracy[((i+1)*(-1))] >= validation_acc:\n","            STEP_SCHEDULER = True\n","        if STEP_SCHEDULER == True:\n","          scheduler.step()\n","          # for g in optimizer.param_groups:\n","          #     g['lr'] =  g['lr']*0.1\n","          print(\"SCHEDULER STEP\")\n","\n","      if len(epoch_val_accuracy) >= 1:\n","        if validation_acc > max(epoch_val_accuracy):\n","          torch.save(model, (log.model_folder + \"/model\"))\n","          print(\"SAVED BEST VALIDATION MODEL WITH ACCURACY: {}\".format(validation_acc))\n","      else:\n","        torch.save(model, (log.model_folder + \"/model\"))\n","        print(\"SAVED BEST VALIDATION MODEL WITH ACCURACY: {}\".format(validation_acc))\n","\n","      epoch_train_accuracy.append(epoch_acc)\n","      epoch_val_accuracy.append(validation_acc)\n","\n","\n","\n","      # if total_accu is not None and total_accu > validation_acc:\n","      #   scheduler.step()\n","      # else:\n","      #   total_accu = validation_acc\n","      print('-' * 59)\n","      print('| end of epoch {:3d} | time: {:5.2f}s | '\n","            'valid accuracy {:8.3f} '.format(epoch,\n","                                            time.time() - epoch_start_time,\n","                                            validation_acc))\n","      print('-' * 59)\n","\n","\n","      log.train_loss.append(epoch_loss)\n","      log.val_loss.append(validation_loss)\n","\n","      log.train_acc.append(epoch_acc)\n","      log.val_acc.append(validation_acc)\n","\n","    model = torch.load((log.model_folder + \"/model\"))\n","\n","    _, accu_test = evaluate(test_dataloader)\n","\n","\n","    #--------------------\n","    #Write Down The data for the logs\n","    #--------------------\n","    data[\"Name\"] = f\"{log.model_name}#{id}\"    \n","    data[f\"{language}-Loss\"] = epoch_loss\n","    data[f\"{language}-train accuracy\"] = epoch_acc\n","    data[f\"{language}-validation accuracy\"] = validation_acc\n","    data[f\"{language}-test accuracy\"] = accu_test\n","    data[\"epoch time\"] = epoch_time\n","\n","\n","\n","    #--------------------\n","    #Get precision recall data\n","    #--------------------\n","    list_label, list_predited_label = get_labels(test_dataloader)\n","    result = classification_report(list_label, list_predited_label, digits=6, output_dict=True)\n","\n","    data[f'{language}-formal-precision'] = result['0']['precision']\n","    data[f'{language}-formal-recall'] = result['0']['recall']\n","    data[f'{language}-formal-f1-score'] = result['0']['f1-score']\n","\n","    data[f'{language}-informal-precision'] = result['1']['precision']\n","    data[f'{language}-informal-recall'] = result['1']['recall']\n","    data[f'{language}-informal-f1-score'] = result['1']['f1-score']\n","\n","  #--------------------\n","  #Dump logs\n","  #--------------------\n","  data_tuples = [(column, data[column]) for column in list(data.keys())]\n","  log.write_data(data_tuples, id)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found a sheet for the model folder\n","Name\n","Epochs\n","Batch size\n","epoch time\n","Description\n","fr-Loss\n","fr-train accuracy\n","fr-validation accuracy\n","fr-test accuracy\n","fr-formal-precision\n","fr-formal-recall\n","fr-formal-f1-score\n","fr-informal-precision\n","fr-informal-recall\n","fr-informal-f1-score\n","pt-Loss\n","pt-train accuracy\n","pt-validation accuracy\n","pt-test accuracy\n","pt-formal-precision\n","pt-formal-recall\n","pt-formal-f1-score\n","pt-informal-precision\n","pt-informal-recall\n","pt-informal-f1-score\n","it-Loss\n","it-train accuracy\n","it-validation accuracy\n","it-test accuracy\n","it-formal-precision\n","it-formal-recall\n","it-formal-f1-score\n","it-informal-precision\n","it-informal-recall\n","it-informal-f1-score\n","all-Loss\n","all-train accuracy\n","all-validation accuracy\n","all-test accuracy\n","all-formal-precision\n","all-formal-recall\n","all-formal-f1-score\n","all-informal-precision\n","all-informal-recall\n","all-informal-f1-score\n","{'Name': 1, 'Epochs': 2, 'Batch size': 3, 'epoch time': 4, 'Description': 5, 'fr-Loss': 6, 'fr-train accuracy': 7, 'fr-validation accuracy': 8, 'fr-test accuracy': 9, 'fr-formal-precision': 10, 'fr-formal-recall': 11, 'fr-formal-f1-score': 12, 'fr-informal-precision': 13, 'fr-informal-recall': 14, 'fr-informal-f1-score': 15, 'pt-Loss': 16, 'pt-train accuracy': 17, 'pt-validation accuracy': 18, 'pt-test accuracy': 19, 'pt-formal-precision': 20, 'pt-formal-recall': 21, 'pt-formal-f1-score': 22, 'pt-informal-precision': 23, 'pt-informal-recall': 24, 'pt-informal-f1-score': 25, 'it-Loss': 26, 'it-train accuracy': 27, 'it-validation accuracy': 28, 'it-test accuracy': 29, 'it-formal-precision': 30, 'it-formal-recall': 31, 'it-formal-f1-score': 32, 'it-informal-precision': 33, 'it-informal-recall': 34, 'it-informal-f1-score': 35, 'all-Loss': 36, 'all-train accuracy': 37, 'all-validation accuracy': 38, 'all-test accuracy': 39, 'all-formal-precision': 40, 'all-formal-recall': 41, 'all-formal-f1-score': 42, 'all-informal-precision': 43, 'all-informal-recall': 44, 'all-informal-f1-score': 45}\n","formal sentences: 103209\n","informal sentences: 100173\n","formal sentences: 10326\n","informal sentences: 9016\n","formal sentences: 21431\n","informal sentences: 19354\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["| epoch 0 \\| 200/398 batches \\| accuracy 0.591758006840796 \\| time elapsed 248.64073634147644 \\| batches/second 0.8043734222429483\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.6699410609037328\n","-----------------------------------------------------------\n","| end of epoch   0 | time: 536.98s | valid accuracy    0.670 \n","-----------------------------------------------------------\n","| epoch 1 \\| 200/398 batches \\| accuracy 0.7012690453980099 \\| time elapsed 248.55733633041382 \\| batches/second 0.8046433187316375\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7466135870127184\n","-----------------------------------------------------------\n","| end of epoch   1 | time: 537.41s | valid accuracy    0.747 \n","-----------------------------------------------------------\n","| epoch 2 \\| 200/398 batches \\| accuracy 0.7405064521144279 \\| time elapsed 245.88915467262268 \\| batches/second 0.8133746291750054\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7719470582152828\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 535.97s | valid accuracy    0.772 \n","-----------------------------------------------------------\n","| epoch 3 \\| 200/398 batches \\| accuracy 0.7564326803482587 \\| time elapsed 249.89028215408325 \\| batches/second 0.8003512512610605\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7740667976424361\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 537.09s | valid accuracy    0.774 \n","-----------------------------------------------------------\n","| epoch 4 \\| 200/398 batches \\| accuracy 0.7670728389303483 \\| time elapsed 246.65826511383057 \\| batches/second 0.8108384282509317\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7744287043739013\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 536.66s | valid accuracy    0.774 \n","-----------------------------------------------------------\n","| epoch 5 \\| 200/398 batches \\| accuracy 0.7741565609452736 \\| time elapsed 249.50169253349304 \\| batches/second 0.8015977686129406\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 538.59s | valid accuracy    0.765 \n","-----------------------------------------------------------\n","| epoch 6 \\| 200/398 batches \\| accuracy 0.7810945273631841 \\| time elapsed 246.55156922340393 \\| batches/second 0.8111893208790617\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7911798159445765\n","-----------------------------------------------------------\n","| end of epoch   6 | time: 540.53s | valid accuracy    0.791 \n","-----------------------------------------------------------\n","| epoch 7 \\| 200/398 batches \\| accuracy 0.7853020055970149 \\| time elapsed 247.6752142906189 \\| batches/second 0.8075091428620814\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   7 | time: 536.08s | valid accuracy    0.776 \n","-----------------------------------------------------------\n","| epoch 8 \\| 200/398 batches \\| accuracy 0.7873328669154229 \\| time elapsed 247.8837172985077 \\| batches/second 0.8068299208178933\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   8 | time: 535.93s | valid accuracy    0.782 \n","-----------------------------------------------------------\n","| epoch 9 \\| 200/398 batches \\| accuracy 0.7911808146766169 \\| time elapsed 249.00017595291138 \\| batches/second 0.8032122838251414\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   9 | time: 535.82s | valid accuracy    0.779 \n","-----------------------------------------------------------\n","formal sentences: 103630\n","informal sentences: 100260\n","formal sentences: 10418\n","informal sentences: 9016\n","formal sentences: 21602\n","informal sentences: 19363\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["| epoch 0 \\| 200/399 batches \\| accuracy 0.5516752176616916 \\| time elapsed 231.71225953102112 \\| batches/second 0.863139483447247\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.6734074302768344\n","-----------------------------------------------------------\n","| end of epoch   0 | time: 508.35s | valid accuracy    0.673 \n","-----------------------------------------------------------\n","| epoch 1 \\| 200/399 batches \\| accuracy 0.6677646921641791 \\| time elapsed 229.9140980243683 \\| batches/second 0.8698901099087984\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7392199238448081\n","-----------------------------------------------------------\n","| end of epoch   1 | time: 511.43s | valid accuracy    0.739 \n","-----------------------------------------------------------\n","| epoch 2 \\| 200/399 batches \\| accuracy 0.7128614738805971 \\| time elapsed 235.39231395721436 \\| batches/second 0.8496454138105487\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7409694350108058\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 512.03s | valid accuracy    0.741 \n","-----------------------------------------------------------\n","| epoch 3 \\| 200/399 batches \\| accuracy 0.7328397077114428 \\| time elapsed 231.64099264144897 \\| batches/second 0.8634050377671052\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.755480086446434\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 508.00s | valid accuracy    0.755 \n","-----------------------------------------------------------\n","| epoch 4 \\| 200/399 batches \\| accuracy 0.742061178482587 \\| time elapsed 236.26247549057007 \\| batches/second 0.8465161451674648\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7651538540701863\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 511.60s | valid accuracy    0.765 \n","-----------------------------------------------------------\n","| epoch 5 \\| 200/399 batches \\| accuracy 0.7471917754975125 \\| time elapsed 234.70790433883667 \\| batches/second 0.8521229847941955\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7659256972316558\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 512.89s | valid accuracy    0.766 \n","-----------------------------------------------------------\n","| epoch 6 \\| 200/399 batches \\| accuracy 0.7521474657960199 \\| time elapsed 231.75406622886658 \\| batches/second 0.8629837795488423\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.767469383554595\n","-----------------------------------------------------------\n","| end of epoch   6 | time: 511.81s | valid accuracy    0.767 \n","-----------------------------------------------------------\n","| epoch 7 \\| 200/399 batches \\| accuracy 0.7568796641791045 \\| time elapsed 232.00977659225464 \\| batches/second 0.8620326390447322\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7707111248327673\n","-----------------------------------------------------------\n","| end of epoch   7 | time: 511.30s | valid accuracy    0.771 \n","-----------------------------------------------------------\n","| epoch 8 \\| 200/399 batches \\| accuracy 0.7617479011194029 \\| time elapsed 235.066015958786 \\| batches/second 0.8508248169529784\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   8 | time: 507.56s | valid accuracy    0.765 \n","-----------------------------------------------------------\n","| epoch 9 \\| 200/399 batches \\| accuracy 0.7625155472636815 \\| time elapsed 233.1110966205597 \\| batches/second 0.8579600152005832\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7737984974786457\n","-----------------------------------------------------------\n","| end of epoch   9 | time: 506.66s | valid accuracy    0.774 \n","-----------------------------------------------------------\n","formal sentences: 103536\n","informal sentences: 100236\n","formal sentences: 10402\n","informal sentences: 9001\n","formal sentences: 21483\n","informal sentences: 19349\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["| epoch 0 \\| 200/398 batches \\| accuracy 0.554580612562189 \\| time elapsed 232.5437626838684 \\| batches/second 0.8600531688819794\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.6858217801370922\n","-----------------------------------------------------------\n","| end of epoch   0 | time: 507.84s | valid accuracy    0.686 \n","-----------------------------------------------------------\n","| epoch 1 \\| 200/398 batches \\| accuracy 0.6700676305970149 \\| time elapsed 231.96320962905884 \\| batches/second 0.8622056933934807\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7585940318507447\n","-----------------------------------------------------------\n","| end of epoch   1 | time: 507.00s | valid accuracy    0.759 \n","-----------------------------------------------------------\n","| epoch 2 \\| 200/398 batches \\| accuracy 0.7154753575870647 \\| time elapsed 233.4915976524353 \\| batches/second 0.8565618720794855\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 502.61s | valid accuracy    0.741 \n","-----------------------------------------------------------\n","| epoch 3 \\| 200/398 batches \\| accuracy 0.7315181902985075 \\| time elapsed 232.69334387779236 \\| batches/second 0.8595003048520267\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 502.81s | valid accuracy    0.753 \n","-----------------------------------------------------------\n","| epoch 4 \\| 200/398 batches \\| accuracy 0.7480760261194029 \\| time elapsed 230.07311987876892 \\| batches/second 0.8692888595824876\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7614286450548884\n","-----------------------------------------------------------\n","| end of epoch   4 | time: 504.69s | valid accuracy    0.761 \n","-----------------------------------------------------------\n","| epoch 5 \\| 200/398 batches \\| accuracy 0.7536633240049752 \\| time elapsed 233.0658791065216 \\| batches/second 0.8581264695060361\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7644694119466062\n","-----------------------------------------------------------\n","| end of epoch   5 | time: 505.36s | valid accuracy    0.764 \n","-----------------------------------------------------------\n","| epoch 6 \\| 200/398 batches \\| accuracy 0.7609608208955224 \\| time elapsed 233.8827769756317 \\| batches/second 0.8551292343379266\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7702932536205741\n","-----------------------------------------------------------\n","| end of epoch   6 | time: 508.07s | valid accuracy    0.770 \n","-----------------------------------------------------------\n","| epoch 7 \\| 200/398 batches \\| accuracy 0.7622046019900498 \\| time elapsed 231.63476586341858 \\| batches/second 0.8634282477179106\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7754986342318199\n","-----------------------------------------------------------\n","| end of epoch   7 | time: 506.81s | valid accuracy    0.775 \n","-----------------------------------------------------------\n","| epoch 8 \\| 200/398 batches \\| accuracy 0.7653043376865671 \\| time elapsed 234.53094220161438 \\| batches/second 0.8527659426195036\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   8 | time: 505.43s | valid accuracy    0.770 \n","-----------------------------------------------------------\n","| epoch 9 \\| 200/398 batches \\| accuracy 0.767616993159204 \\| time elapsed 231.69626188278198 \\| batches/second 0.8631990795828312\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7774055558418801\n","-----------------------------------------------------------\n","| end of epoch   9 | time: 507.00s | valid accuracy    0.777 \n","-----------------------------------------------------------\n","formal sentences: 414354\n","informal sentences: 401055\n","formal sentences: 41856\n","informal sentences: 36064\n","formal sentences: 86648\n","informal sentences: 77514\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["| epoch 0 \\| 200/1593 batches \\| accuracy 0.5836637126865671 \\| time elapsed 251.63027572631836 \\| batches/second 0.7948169170927858\n","| epoch 0 \\| 400/1593 batches \\| accuracy 0.6248879753740648 \\| time elapsed 244.79160118103027 \\| batches/second 0.8170214951618964\n","| epoch 0 \\| 600/1593 batches \\| accuracy 0.6502021370632279 \\| time elapsed 249.01474952697754 \\| batches/second 0.8031652758718719\n","| epoch 0 \\| 800/1593 batches \\| accuracy 0.6649281171972534 \\| time elapsed 248.38311958312988 \\| batches/second 0.8052076982351579\n","| epoch 0 \\| 1000/1593 batches \\| accuracy 0.6758905157342657 \\| time elapsed 243.8757266998291 \\| batches/second 0.8200898166719441\n","| epoch 0 \\| 1200/1593 batches \\| accuracy 0.6839775447543713 \\| time elapsed 244.40761351585388 \\| batches/second 0.8183051138340528\n","| epoch 0 \\| 1400/1593 batches \\| accuracy 0.6902728519807281 \\| time elapsed 245.07340025901794 \\| batches/second 0.8160820382327095\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7417094455852156\n","-----------------------------------------------------------\n","| end of epoch   0 | time: 2178.39s | valid accuracy    0.742 \n","-----------------------------------------------------------\n","| epoch 1 \\| 200/1593 batches \\| accuracy 0.7379703047263682 \\| time elapsed 252.583758354187 \\| batches/second 0.7918165495009734\n","| epoch 1 \\| 400/1593 batches \\| accuracy 0.7397765352244389 \\| time elapsed 243.54911756515503 \\| batches/second 0.8211895900074258\n","| epoch 1 \\| 600/1593 batches \\| accuracy 0.7410110752911814 \\| time elapsed 248.99721598625183 \\| batches/second 0.8032218320506959\n","| epoch 1 \\| 800/1593 batches \\| accuracy 0.7418217462546817 \\| time elapsed 249.49468660354614 \\| batches/second 0.8016202778610891\n","| epoch 1 \\| 1000/1593 batches \\| accuracy 0.7424548108141859 \\| time elapsed 245.96710085868835 \\| batches/second 0.8131168733614618\n","| epoch 1 \\| 1200/1593 batches \\| accuracy 0.7423972861157369 \\| time elapsed 246.0166449546814 \\| batches/second 0.8129531237077146\n","| epoch 1 \\| 1400/1593 batches \\| accuracy 0.7429584337080657 \\| time elapsed 247.15565180778503 \\| batches/second 0.8092066620250369\n","SCHEDULER STEP\n","SAVED BEST VALIDATION MODEL WITH ACCURACY: 0.7832777207392198\n","-----------------------------------------------------------\n","| end of epoch   1 | time: 2180.26s | valid accuracy    0.783 \n","-----------------------------------------------------------\n","| epoch 2 \\| 200/1593 batches \\| accuracy 0.7592603389303483 \\| time elapsed 249.13380670547485 \\| batches/second 0.8027814556554315\n","| epoch 2 \\| 400/1593 batches \\| accuracy 0.761241427680798 \\| time elapsed 251.01597952842712 \\| batches/second 0.79676202437682\n","| epoch 2 \\| 600/1593 batches \\| accuracy 0.7631031613976705 \\| time elapsed 243.77030086517334 \\| batches/second 0.8204444893006789\n","| epoch 2 \\| 800/1593 batches \\| accuracy 0.7638840121722846 \\| time elapsed 247.13752555847168 \\| batches/second 0.8092660131157656\n","| epoch 2 \\| 1000/1593 batches \\| accuracy 0.7643860046203796 \\| time elapsed 244.67532563209534 \\| batches/second 0.8174097632579791\n","| epoch 2 \\| 1200/1593 batches \\| accuracy 0.7648492792464613 \\| time elapsed 244.48535466194153 \\| batches/second 0.8180449102014598\n","| epoch 2 \\| 1400/1593 batches \\| accuracy 0.7649154064061384 \\| time elapsed 248.9972152709961 \\| batches/second 0.8032218343579868\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   2 | time: 2175.87s | valid accuracy    0.775 \n","-----------------------------------------------------------\n","| epoch 3 \\| 200/1593 batches \\| accuracy 0.7741759950248757 \\| time elapsed 249.3031747341156 \\| batches/second 0.8022360734607654\n","| epoch 3 \\| 400/1593 batches \\| accuracy 0.7747671836034913 \\| time elapsed 247.83606100082397 \\| batches/second 0.8069850658227459\n","| epoch 3 \\| 600/1593 batches \\| accuracy 0.7743896890599001 \\| time elapsed 246.40067553520203 \\| batches/second 0.8116860863533916\n","| epoch 3 \\| 800/1593 batches \\| accuracy 0.7744616104868914 \\| time elapsed 249.5472333431244 \\| batches/second 0.8014514820326717\n","| epoch 3 \\| 1000/1593 batches \\| accuracy 0.7744208916083916 \\| time elapsed 253.03502464294434 \\| batches/second 0.7904044125204341\n","| epoch 3 \\| 1200/1593 batches \\| accuracy 0.7751596976477935 \\| time elapsed 247.45807552337646 \\| batches/second 0.8082177135540954\n","| epoch 3 \\| 1400/1593 batches \\| accuracy 0.7752107869379015 \\| time elapsed 246.46142601966858 \\| batches/second 0.8114860131663736\n","SCHEDULER STEP\n","-----------------------------------------------------------\n","| end of epoch   3 | time: 2185.25s | valid accuracy    0.773 \n","-----------------------------------------------------------\n","| epoch 4 \\| 200/1593 batches \\| accuracy 0.781356887437811 \\| time elapsed 252.80045318603516 \\| batches/second 0.7911378222602337\n"]}]},{"cell_type":"code","metadata":{"id":"uumkUMxrghZ6"},"source":["from os import listdir\n","from os.path import isfile, join\n","drive.mount('/content/drive/')\n","root_path = '/content/drive/My Drive/Colab Notebooks/Sentiment Classifier/'\n","onlyfiles = [f for f in listdir(root_path) if isfile(join(root_path, f))]\n","print(filenames)"],"execution_count":null,"outputs":[]}]}