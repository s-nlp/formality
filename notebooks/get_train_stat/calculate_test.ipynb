{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5985c7f-fbb7-4981-b58d-e1d00c7f5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a82c4b1-7918-42c7-82cd-fc7d187c0f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 17:16:46.854965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 17:16:47.633392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from data import load_gyafc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import (DebertaTokenizer,\n",
    "                          DebertaForSequenceClassification,\n",
    "                          AutoModelForSequenceClassification,  \n",
    "                          AutoTokenizer,\n",
    "                          Trainer,TrainingArguments)\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c330b251-cb5e-4d89-a788-b655a72dadc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt5_utils import MT5ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c73558c-7498-4df8-be2f-9574491fadc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f227d06-a4da-479c-85f2-5744663fc5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.28.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5d22f4-284e-4393-9b6f-b8525c9767f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ede00a-ad93-40bb-8497-a5efb9d65681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_dataset(model_name):\n",
    "    tr_val_test_datasets = load_gyafc(model_name, toy = False)\n",
    "    _, _, test_dataset = tr_val_test_datasets\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "589d8ef4-a71c-446a-a454-d52184d03447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_type(model_name):\n",
    "    if \"deberta-large\" in model_name:\n",
    "        return \"microsoft/deberta-large\"\n",
    "    elif \"deberta-base\" in model_name:\n",
    "        return \"microsoft/deberta-base\"\n",
    "    elif \"bigscience_bloom-560m\" in model_name:\n",
    "        return \"bigscience/bloom-560m\"\n",
    "    elif \"mt5-base\" in model_name:\n",
    "        return \"google/mt5-base\"\n",
    "    \n",
    "    elif \"distilbert-base-multilingual-cased\" in model_name:\n",
    "        return \"distilbert-base-multilingual-cased\"\n",
    "    \n",
    "    elif \"bert-base-multilingual-cased\" in model_name:\n",
    "        return \"bert-base-multilingual-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c06e93d3-b7b2-40c9-a5ea-f1f60af7aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_dict = {}\n",
    "# for mt in [\"microsoft/deberta-large\",\"microsoft/deberta-base\",\"bigscience/bloom-560m\",\"google/mt5-base\"]:\n",
    "#     test_dataset_dict[mt] = get_test_dataset(mt)\n",
    "    \n",
    "for mt in [\"distilbert-base-multilingual-cased\"]:\n",
    "    test_dataset_dict[mt] = get_test_dataset(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601b72cc-a29b-429a-97f4-3c131e435ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distilbert-base-multilingual-cased': <data.Formal_informal at 0x7ff4d8cc4610>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc25053-caf5-47f7-bd8d-31e87da007fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-multilingual-cased'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50471f75-5766-4b67-ab67-b1c7a9cb4a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped bert-base-multilingual-cased_ep5_wus1598_lr1e-06_batch32\n",
      "dropped google_mt5-base_ep5_wus2046_lr1e-05_batch25\n",
      "dropped microsoft_deberta-large_ep5_wus2000_lr1e-05\n",
      "dropped bigscience_bloom-560m_ep5_wus6394_lr1e-06_batch8\n",
      "dropped microsoft_deberta-large_ep5_wus12789_lr1e-06_batch16\n",
      "dropped google_mt5-base_ep5_wus6394_lr1e-05_batch16\n",
      "distilbert-base-multilingual-cased_ep5_wus1598_lr1e-06_batch32\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped microsoft_deberta-base_ep10_wus2000_lr0.0001\n",
      "dropped bigscience_bloom-560m_ep5_wus12789_lr1e-06_batch8\n",
      "dropped microsoft_deberta-large_ep5_wus100_lr1e-05\n",
      "dropped google_mt5-base_ep5_wus3197_lr1e-06_batch16\n",
      "dropped bigscience_bloom-560m_ep3_wus6394_lr1e-05_batch8\n",
      "dropped bert-base-multilingual-cased_ep5_wus1598_lr1e-05_batch32\n",
      "dropped microsoft_deberta-large_ep5_wus6394_lr1e-06_batch16\n",
      "dropped microsoft_deberta-large_ep5_wus19184_lr1e-06_batch16\n",
      "dropped microsoft_deberta-large_ep5_wus2000_lr1e-06\n",
      "dropped microsoft_deberta-large_ep5_wus100_lr1e-06\n",
      "dropped bigscience_bloom-560m_ep5_wus12789_lr1e-05_batch8\n",
      "dropped .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "trained_models_fld = \"../trained_models/\"\n",
    "\n",
    "existing_inference_results = os.listdir(\"test_results_trainer/\")\n",
    "\n",
    "for model_folder in os.listdir(trained_models_fld) :\n",
    "    \n",
    "    if f\"{model_folder}.json\" not in existing_inference_results and model_folder != \".ipynb_checkpoints\":\n",
    "        \n",
    "        print(model_folder)\n",
    "        \n",
    "        model_type = get_model_type(model_folder)\n",
    "        current_test_dataset = test_dataset_dict[model_type]      \n",
    "        \n",
    "        model_folder_abs = os.path.join(trained_models_fld, model_folder)\n",
    "        try:\n",
    "            test_model = AutoModelForSequenceClassification.from_pretrained(f\"{model_folder_abs}/nli_model/\")\n",
    "        except ValueError:\n",
    "            test_model = MT5ForSequenceClassification.from_pretrained(f\"{model_folder_abs}/nli_model/\")\n",
    "        \n",
    "        training_args = TrainingArguments(per_device_eval_batch_size=64, output_dir = \"./tmp_trainer/\", dataloader_drop_last = True)\n",
    "        \n",
    "        trainer = Trainer(model=test_model, args = training_args)\n",
    "        \n",
    "        trainer_preds = trainer.predict(current_test_dataset)\n",
    "        \n",
    "        list_predited_label = torch.max(torch.tensor(trainer_preds.predictions), dim=1).indices.tolist()    \n",
    "        \n",
    "        with open(f\"test_results_trainer/{model_folder}.json\", \"w\") as f:\n",
    "                json.dump(list_predited_label, f)\n",
    "    else:\n",
    "        print(\"dropped\", model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f8025-97f6-4829-bf98-4eab736df091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2270984-ae27-4aef-8c1f-c3832ce87a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3e264-11f0-4657-8893-287355a09caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old vers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523bc91-01d2-4500-a4bc-8131981919f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models_fld = \"../trained_models/\"\n",
    "\n",
    "existing_inference_results = os.listdir(\"test_results/\")\n",
    "\n",
    "for model_folder in os.listdir(trained_models_fld):\n",
    "    \n",
    "    if f\"{model_folder}.json\" not in existing_inference_results:\n",
    "        \n",
    "        print(model_folder)\n",
    "        \n",
    "        model_type = get_model_type(model_folder)\n",
    "        current_test_dataset = test_dataset_dict[model_type]      \n",
    "        \n",
    "        test_dataloader = DataLoader(\n",
    "            current_test_dataset,\n",
    "            batch_size=64,\n",
    "            # num_workers=4,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        model_folder_abs = os.path.join(trained_models_fld, model_folder)        \n",
    "        test_model = AutoModelForSequenceClassification.from_pretrained(f\"{model_folder_abs}/nli_model/\")\n",
    "        test_model = test_model.to(device)\n",
    "        \n",
    "\n",
    "        list_predited_label = []\n",
    "        list_label = []\n",
    "        with torch.no_grad():\n",
    "            for d in tqdm(test_dataloader):\n",
    "\n",
    "                input_ids = d[\"input_ids\"].to(device) # .reshape(64, 24)\n",
    "                attention_mask = d[\"attention_mask\"].to(device)\n",
    "\n",
    "                outputs = test_model(input_ids, attention_mask)\n",
    "                logits = outputs[0]\n",
    "\n",
    "                _, prediction = torch.max(logits, dim=1)\n",
    "                targets = d[\"labels\"].detach().numpy().tolist()\n",
    "                prediction = prediction.cpu().detach().numpy().tolist()\n",
    "\n",
    "                list_label.extend(targets)\n",
    "                list_predited_label.extend(prediction)\n",
    "\n",
    "            with open(f\"test_results/{model_folder}.json\", \"w\") as f:\n",
    "                json.dump(list_predited_label, f)\n",
    "    else:\n",
    "        print(\"dropped\", model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db3575-cde3-4e9f-825d-6e0fd9dc7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"GYAFC_test_labels.json\", \"w\") as f:\n",
    "#     json.dump(list_label, f)\n",
    "\n",
    "# len(list_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
